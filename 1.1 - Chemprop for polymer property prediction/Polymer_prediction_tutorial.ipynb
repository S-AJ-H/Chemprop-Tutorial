{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Polymer prediction with Chemprop\n",
        "\n",
        "Sam A.J. Hillman Feb 2025."
      ],
      "metadata": {
        "id": "JixXjxzy16Ja"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3khTEFSKnBNS"
      },
      "source": [
        "# 1) Install Chemprop from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4eAu_qknBNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f2487f0-c660-45ae-9cdc-d8b049d52ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chemprop'...\n",
            "remote: Enumerating objects: 24696, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n"
          ]
        }
      ],
      "source": [
        "# Install chemprop from GitHub if running in Google Colab\n",
        "import os\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "    try:\n",
        "        import chemprop\n",
        "    except ImportError:\n",
        "        !git clone https://github.com/chemprop/chemprop.git\n",
        "        %cd chemprop\n",
        "        !pip install .\n",
        "\n",
        "#Import packages\n",
        "from pathlib import Path\n",
        "\n",
        "import lightning.pytorch as pl\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from rdkit import Chem                  #rdkit is used to convert SMILES to mols\n",
        "from rdkit.Chem import Draw#, PandasTools  #PandasTools enables using RDKit molecules in columns of a Pandas dataframe\n",
        "from rdkit.Chem.Draw import SimilarityMaps  #for drawing the partial charges\n",
        "from chemprop import data, featurizers, models, nn    #chemprop is our GNN package\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrEf3rQznBNW"
      },
      "source": [
        "# 2) Load, explore and process data\n",
        "\n",
        "> Pair the acids and bromides together to make 9 * 682 =6138 unique polymers.\n",
        "\n",
        "> When the monomers react together, the bonding occurs where the  Br and B(OH)(OH) groups are, so these fall off. One of the benefits of the mol representation is that it is easy to replicate this chemistry virtually with RDKit (this is more robust than editing SMILES strings).\n",
        "\n",
        ">In this case, the reaction has been done for us - all polymers are in  \"polymer_dataset_alternating.csv\"\n",
        "\n",
        "> All molecules can be written with SMILES (Simplified Molecular Input Line Entry System).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwfsGfIML2ix"
      },
      "outputs": [],
      "source": [
        "#Get the polymer SMILES from GitHub.\n",
        "csv_url = \"https://raw.githubusercontent.com/S-AJ-H/Chemprop-Tutorial/main/polymer_dataset_alternating.csv\"  #get data\n",
        "df = pd.read_csv(csv_url) # Load into a DataFrame\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (i) Isolated example: Drawing the pairs of monomers (the \"polymers\")\n",
        "\n",
        "> This example is for demonstration purposes and isn't used in the model.\n",
        "\n",
        "> We construct 'molecule' objects from the SMILES using RDKit. RDKit uses a collection of rules to calculate a complete set of molecule-defining chemical information from the SMILES. These molecule objects encode the atomic structure, bonds, spatial arrangement etc of a molecule.\n",
        "\n",
        "> We can still represent pairs of monomers as single 'molecule' objects. This approach has some problems (e.g. there is no explict info on where or how the bonding takes place) but is OK for now.\n",
        "\n",
        "> RDKit: https://www.rdkit.org/docs/index.html\n",
        "\n"
      ],
      "metadata": {
        "id": "GSNtbEjzVNQS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBmagKRgbHML"
      },
      "outputs": [],
      "source": [
        "# Lets look at a few randomly chosen mol objects representing pairs of monomers.\n",
        "\n",
        "#Convert the SMILES to mol objects using MolFromSmiles on each element in the Series:\n",
        "random_mols_index = [0,1000,2000,5000, 6137]\n",
        "df['poly_MOL'] = df['poly_SMI'].iloc[random_mols_index].apply(Chem.MolFromSmiles)   #Chem.MolFromSmiles is the RDKit function\n",
        "\n",
        "#Draw the pairs:\n",
        "img = Draw.MolsToGridImage(list(df.poly_MOL.iloc[random_mols_index]), molsPerRow=5)\n",
        "display(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDKit gives us chemistry! As an example, lets calculate the partial charges for the first pair of monomers. The resulting graph shows where electrons are localised in the molecules (blue = higher electron density, brown = lower)."
      ],
      "metadata": {
        "id": "6K5yCgR-uStt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RDKit allows us to do loads of chemistry. Lets look at the charge localisation of the first pair of monomers\n",
        "\n",
        "#calculate:\n",
        "mol = df.poly_MOL.iloc[0]   #access first pair of monomers\n",
        "Chem.AllChem.ComputeGasteigerCharges(mol) #calculate the partial charges\n",
        "contribs = [mol.GetAtomWithIdx(i).GetDoubleProp('_GasteigerCharge') for i in range(mol.GetNumAtoms())]  #store in contribs with the atom index\n",
        "\n",
        "#now draw:\n",
        "d2d = Draw.MolDraw2DCairo(400, 400)\n",
        "drawing = Draw.SimilarityMaps.GetSimilarityMapFromWeights(mol, contribs, draw2d=d2d, colorMap='jet', contourLines=10)\n",
        "drawing.FinishDrawing()\n",
        "\n",
        "import io\n",
        "from PIL import Image\n",
        "def show_png(data):\n",
        "    bio = io.BytesIO(data)\n",
        "    img = Image.open(bio)\n",
        "    return img\n",
        "\n",
        "show_png(drawing.GetDrawingText())  #Note how the bottom OH has a dotted line around it which is influenced by the bottom molecule"
      ],
      "metadata": {
        "id": "wfeImt5RjVij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vYicChOnBNX"
      },
      "source": [
        "## 2a) Make molecule datapoints\n",
        "\n",
        "> Molecule datapoints link the molecule objects, being the RDKit-generated molecules, and the target(s) y.\n",
        "\n",
        "> It also adds extra things which Chemprop can use later (such as opportunities to manually add extra properties).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv1P6C1vnBNX"
      },
      "outputs": [],
      "source": [
        "#Get SMILES and targets\n",
        "smiles = df.loc[:, 'poly_SMI'].values\n",
        "targets = df.loc[:,['EA']].values\n",
        "\n",
        "display(smiles[:2]) # show first 2 SMILES strings\n",
        "display(targets[:2]) # show first 2 targets\n",
        "\n",
        "#Use the SMILES to generate mol objects, pair the mol objects with the targets y\n",
        "all_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(smiles, targets)]\n",
        "display(all_data[:2])\n",
        "\n",
        "#We can still access the molecules and extract chemical info if we like:\n",
        "#all_data[0].mol.GetNumAtoms()   #Can use RDKit things on the mol, but not on the MoleculeDatapoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylGNHyFXnBNY"
      },
      "source": [
        "## (ii) Isolated example: Featurization with RDKit\n",
        "\n",
        ">To start the training, we need \"MoleculeDatasets\". These comprise a list of Molecules and ends with the choice of featurizer. We need one MoleculeDataset for each of the training, validation and testing splits.\n",
        "\n",
        "> Here we use the built in featurizer \"SimpleMoleculeMolGraphFeaturizer\", which is a part of RDKit. It outputs a \"MolGraph\", which is the graph featurisation of the molecule i.e. atom and bond features. These features will be used to kick-start the upcoming message passing.\n",
        "\n",
        ">SimpleMoleculeMolGraphFeaturizer uses a multi-hot encoding to featurize individual atoms and bonds. In the example below for carbon monoxide (CO), you can see arrays for V (atom features) and E (edge/bond features), along with a mapping between atoms and bonds (edge_index and rev_edge_index).\n",
        "\n",
        ">Atom features are generated by rdkit and cast to one-hot vectors. Features include e.g. the mass, charge, number of bonded hydrogen atoms (for atoms); bond type, conjugation, whether its in an aromatic ring (for bonds). These feature vectors are joined together to a single multi-hot feature vector.\n",
        "\n",
        ">https://chemprop.readthedocs.io/en/latest/tutorial/python/featurizers/molgraph_molecule_featurizer.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2aD_ukhnBNY"
      },
      "outputs": [],
      "source": [
        "#lets look at the featurizer:\n",
        "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer() #chemprop module which outputs a MolGraph\n",
        "\n",
        "#start with a simple two-atom example:\n",
        "carbon_monoxide = Chem.MolFromSmiles(\"[C-]#[O+]\")\n",
        "display(Draw.MolToImage(carbon_monoxide))\n",
        "\n",
        "display(\"Features of carbon monoxide:\", featurizer(carbon_monoxide))  #see e.g. 0.12011 and 0.15999 at the end of the arrays, which are the (normalised) masses\n",
        "\n",
        "#now lets have alook at the features of the first pair of monomers in the training dataset:\n",
        "#display(\"Features of the first polymer:\", featurizer(train_data[0][0].mol))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq_WbgLmnBNY"
      },
      "source": [
        "## 2b) Perform data splitting for training, validation, and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FevWp_9LnBNY"
      },
      "source": [
        "ChemProp's `make_split_indices` function will always return a two- (if no validation) or three-length tuple (if including validation, like in this example). The inner lists then contain the actual indices for splitting.\n",
        "\n",
        "The type signature for this return type is `tuple[list[list[int]], ...]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5paMtwC8nBNY"
      },
      "outputs": [],
      "source": [
        "# available split types - Kennard Stone, Kmeans and scafford balanced are all structure-based splits (similar structures go to same split) based on https://jacksonburns.github.io/astartes/\n",
        "#list(data.SplitType.keys())\n",
        "\n",
        "mols = [d.mol for d in all_data]  # RDkit Mol objects are use for structure based splits\n",
        "train_indices, val_indices, test_indices = data.make_split_indices(mols, \"random\", (0.8, 0.1, 0.1))  # unpack the tuple into three separate lists. data is a Chemprop function\n",
        "\n",
        "#display(test_indices[0]) #test indices\n",
        "display(\"test_indices length =\", len(test_indices[0]))     #~6138/10. (Note that the list of test indices is nested)\n",
        "\n",
        "train_data, val_data, test_data = data.split_data_by_indices(\n",
        "    all_data, train_indices, val_indices, test_indices)     #Use the 3 lists of indices to split the data.\n",
        "\n",
        "#display(len(test_data[0]))\n",
        "#display(test_data[0][0])  #same format as in 2a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2c) Make the three MoleculeDatasets"
      ],
      "metadata": {
        "id": "evggn0vsZrPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dset = data.MoleculeDataset(train_data[0], featurizer)  #append the type of featuriser to the list of molecule to make the molecule dataset. (We use train_data[0] because of the aforementioned nesting)\n",
        "#The MoleculeDataset \"train_dset\" is a list of MoleculeDatapoints, with the featurizer type listed at the end:\n",
        "#display(len(test_dset))\n",
        "#display(test_dset.data[:1])      #test_dset.data is the same as test_data, without nesting;\n",
        "\n",
        "#each MoleculeDatapoint now has MolGraph features which are accessed through indexing. Compare to Part (4) - its the same but we have MolGraph features instead of mol objects.\n",
        "#display(test_dset[0])\n",
        "\n",
        "scaler = train_dset.normalize_targets() #define the normalisation using StandardScaler (subtract mean, scale to unit variance)\n",
        "\n",
        "\n",
        "#Do the same for validation and test\n",
        "val_dset = data.MoleculeDataset(val_data[0], featurizer)\n",
        "val_dset.normalize_targets(scaler)            #normalise the validation dataset in the same way as the training\n",
        "\n",
        "test_dset = data.MoleculeDataset(test_data[0], featurizer)      #no normalisation\n"
      ],
      "metadata": {
        "id": "OGccGzfyX_pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GEuFA8GnBNY"
      },
      "source": [
        "# 3) Message-Passing Neural Network input parameters\n",
        "\n",
        ">Now our data is ready, its time to use Chemprop.There are 3 main steps: Message passing, aggregation and the feed-forward NN.\n",
        "\n",
        "> For more info on a step: https://chemprop.readthedocs.io/en/latest/tutorial/python/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-3oQW3ynBNY"
      },
      "source": [
        "## 3a) Message Passing and Aggregation\n",
        "\n",
        "`Message passing`: Constructs hidden node-level representations. Pass messages from bond to bond or atom to atom. Bond to bond allows for directed messages and is generally preferred. Options are `mp = nn.BondMessagePassing()` or `mp = nn.AtomMessagePassing()`.\n",
        "\n",
        "`Aggregation`: The aggregation layer combines the node level representations into a graph level representation (usually atoms -> molecule). Options include - `agg = nn.MeanAggregation()`, `agg = nn.SumAggregation()`, `agg = nn.NormAggregation()`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Message passing: construct node-level representations of the atoms\n",
        "#Define message passing type. Can pass different activations, dropout, etc etc.\n",
        "#Defaults: 300 hidden dimensions. 3 message passing iterations. ReLU.\n",
        "mp = nn.BondMessagePassing()    #https://chemprop.readthedocs.io/en/latest/autoapi/chemprop/nn/index.html#chemprop.nn.BondMessagePassing\n",
        "\n",
        "#Aggregation: node-level --> graph-level representation\n",
        "agg = nn.MeanAggregation()    #average together all of the hidden node/edge representations to get a graph-level representation.\n",
        "#Note this is the only place where the two monomers in each pair interact!\n",
        "batch_norm = True #normalizes the outputs of the aggregation by re-centering and re-scaling. Helps keep the inputs to the FFN small and centered around zero."
      ],
      "metadata": {
        "id": "yqD93OQlyIQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9khOv4FnBNZ"
      },
      "source": [
        "## 3b) Feed-Forward Network (FFN)\n",
        "\n",
        "A `FFN` takes the aggregated representations and make target predictions.\n",
        "\n",
        "Regression options include:\n",
        "- `ffn = nn.RegressionFFN()`\n",
        "- `ffn = nn.MveFFN()`\n",
        "- `ffn = nn.EvidentialFFN()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wKSfvsfnBNZ"
      },
      "outputs": [],
      "source": [
        "output_transform = nn.UnscaleTransform.from_standard_scaler(scaler) #unscale the data\n",
        "\n",
        "#define the feed-forward network:\n",
        "ffn = nn.RegressionFFN(output_transform=output_transform, n_tasks = 1) #n_tasks sets the number of targets. Can change number of layers etc etc.\n",
        "#set the metrics:\n",
        "metric_list = [nn.metrics.RMSE(), nn.metrics.MAE(), nn.metrics.R2Score()] # Only the first metric is used for training and early stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHAy3WEhnBNc"
      },
      "source": [
        "## 3c) Construct the MPNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn5LZOANnBNc"
      },
      "outputs": [],
      "source": [
        "mpnn = models.MPNN(mp, agg, ffn, batch_norm, metric_list) #can change learning rate, optimiser etc\n",
        "mpnn\n",
        "#Entire model, consisting of message passing, aggregation and FFN, is end-to-end trained.\n",
        "\n",
        "#In the message passing NN:\n",
        "#w_i = input weights, applied to the bond feature vectors. Length = sum of bond and atom features. Output hidden dimension is 300 by default.\n",
        "#w_h = hidden weights, applied to the messages.\n",
        "#w_o = output weights.\n",
        "\n",
        "#In the FFN:\n",
        "#Input dimension is the same as the MPNN hidden dimension (300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siESRK6CnBNd"
      },
      "source": [
        "# 4) Set up and start the trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qanCRbNRnBNd"
      },
      "outputs": [],
      "source": [
        "#Get DataLoader\n",
        "\n",
        "num_workers = 0 # number of workers for dataloader. 0 means using main process for data loading. Specifies how many subprocesses should be used to load data.\n",
        "#Each of these subprocesses retrieves a batch of data from your dataset and sends it to the main training process.\n",
        "\n",
        "train_loader = data.build_dataloader(train_dset, num_workers=num_workers)\n",
        "val_loader = data.build_dataloader(val_dset, num_workers=num_workers, shuffle=False)\n",
        "test_loader = data.build_dataloader(test_dset, num_workers=num_workers, shuffle=False)\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "# Configure model checkpointing\n",
        "checkpointing = ModelCheckpoint(\n",
        "    \"checkpoints\",  # Directory where model checkpoints will be saved\n",
        "    \"best-{epoch}-{val_loss:.2f}\",  # Filename format for checkpoints, including epoch and validation loss\n",
        "    \"val_loss\",  # Metric used to select the best checkpoint (based on validation loss)\n",
        "    mode=\"min\",  # Save the checkpoint with the lowest validation loss (minimization objective)\n",
        "    save_last=True,  # Always save the most recent checkpoint, even if it's not the best\n",
        ")\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=False,\n",
        "    enable_checkpointing=True, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
        "    enable_progress_bar=True,\n",
        "    accelerator=\"auto\",\n",
        "    devices=1,\n",
        "    max_epochs=10, # number of epochs to train for\n",
        "    callbacks=[checkpointing], # Use the configured checkpoint callback\n",
        ")\n",
        "\n",
        "\n",
        "#start training\n",
        "trainer.fit(mpnn, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ebdbmalnBNe"
      },
      "source": [
        "# 5) Test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2W_an7InBNe"
      },
      "outputs": [],
      "source": [
        "results = trainer.test(dataloaders=test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTgbvval2uK0"
      },
      "outputs": [],
      "source": [
        "full_dset = data.MoleculeDataset(all_data, featurizer=featurizer)\n",
        "full_loader = data.build_dataloader(full_dset, shuffle=False)\n",
        "\n",
        "predictions = trainer.predict(mpnn, full_loader)\n",
        "\n",
        "#append the predicted values to the original array\n",
        "predictions_array = np.concatenate(predictions, axis=0)\n",
        "df[['pred_EA']] = predictions_array\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo85FpHe7xAw"
      },
      "source": [
        "Plot prediction vs actual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkqILBHV7u3d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot pred vs true\n",
        "plt.scatter(df['EA'], df['pred_EA'], label='Prediction', marker='x', color='red')\n",
        "\n",
        "# Plot y=x\n",
        "plt.plot(df['EA'], df['EA'], label='y=x', linestyle='-')\n",
        "\n",
        "plt.xlabel('true EA')\n",
        "plt.ylabel('Predicted EA')\n",
        "plt.title('Pred vs true EA')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Bonus: Transfer learning\n",
        "\n",
        "Transfer learning (or pretraining) leverages knowledge from a pre-trained model on a related task to enhance performance on a new task. In Chemprop, we can use pre-trained model checkpoints to initialize a new model and freeze components of the new model during training.\n",
        "\n",
        "The originally imported data contains a column \"IP\". By repeating the steps above, can you train a model to a similar level of accuracy to the one above using a smaller training dataset?"
      ],
      "metadata": {
        "id": "Q_zuK_pRYtaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "7GI7CRvxdoCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6a) Make the MoleculeDatasets by following the steps in Section 2.\n",
        "\n",
        "> The previous model saved a checkpoint file with all of the weights etc from the trained MPNN. We need to call it between 2b and 2c below. We also need to ensure that the scaling is the same as that used in the first model."
      ],
      "metadata": {
        "id": "y2PCnUHOegAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2a as before\n",
        "\n",
        "#2b as before\n",
        "\n",
        "#new bit: the previous model saved a checkpoint file with all of the weights etc from the trained MPNN. We can call it here:\n",
        "checkpoint_path = \"/content/chemprop/checkpoints/best-epoch=7-val_loss=0.03.ckpt\" #replace with your checkpoint name (see the files on the left hand side)\n",
        "mpnn_cls = models.MPNN\n",
        "mpnn = mpnn_cls.load_from_file(checkpoint_path)\n",
        "mpnn\n",
        "\n",
        "#new bit: scaling. Need to use the same scaling as in the pre-trained data.\n",
        "pretraining_scaler = scaler\n",
        "pretraining_scaler.mean_ = mpnn.predictor.output_transform.mean.numpy()\n",
        "pretraining_scaler.scale_ = mpnn.predictor.output_transform.scale.numpy()\n",
        "\n",
        "#2c as before"
      ],
      "metadata": {
        "id": "LZpXZzzSd1Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9UXMiGMdIFJ"
      },
      "source": [
        "## 6b) Freezing MPNN and FFN layers\n",
        "Certain layers of a pre-trained model can be kept unchanged during further training on a new task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlyBAmm2dIFJ"
      },
      "outputs": [],
      "source": [
        "#To freeze the MPNN (i.e. the learned representation of the molecules)\n",
        "mpnn.message_passing.apply(lambda module: module.requires_grad_(False))\n",
        "mpnn.message_passing.eval()\n",
        "mpnn.bn.apply(lambda module: module.requires_grad_(False))\n",
        "mpnn.bn.eval()  # Set batch norm layers to eval mode to freeze running mean and running var."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6c) Train and test by following the steps in Sections 4 and 5.\n",
        "\n",
        "> Are the predictions for IP better or worse than for EA?\n",
        "\n",
        "> Change the data split such that you are only training on 10% of the data (Try running for 50 epochs instead of 10)."
      ],
      "metadata": {
        "id": "3FpV1is_gCmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Train. Notice how there are fewer trainable parameters in the model now.\n",
        "\n",
        "#5 Results."
      ],
      "metadata": {
        "id": "NPE9oUn0gLNL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
