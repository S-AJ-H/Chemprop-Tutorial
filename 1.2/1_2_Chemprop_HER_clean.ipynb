{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3khTEFSKnBNS"
      },
      "source": [
        "# 1) Install Chemprop from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4eAu_qknBNU"
      },
      "outputs": [],
      "source": [
        "# Install chemprop from GitHub if running in Google Colab\n",
        "import os\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "    try:\n",
        "        import chemprop\n",
        "    except ImportError:\n",
        "        !git clone https://github.com/chemprop/chemprop.git\n",
        "        %cd chemprop\n",
        "        !pip install -qq .\n",
        "\n",
        "#Import packages\n",
        "from pathlib import Path\n",
        "\n",
        "import lightning.pytorch as pl\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from rdkit import Chem                  #rdkit is used to convert SMILES to mols\n",
        "from rdkit.Chem import Draw#, PandasTools  #PandasTools enables using RDKit molecules in columns of a Pandas dataframe\n",
        "from rdkit.Chem.Draw import SimilarityMaps  #for drawing the partial charges\n",
        "from chemprop import data, featurizers, models, nn    #chemprop is our GNN package\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrEf3rQznBNW"
      },
      "source": [
        "# 2) Load, process and explore data\n",
        "\n",
        "> In this part, we will use an experimental dataset containing ~ 70 polymers.\n",
        "Data: https://pubs.acs.org/doi/full/10.1021/jacs.9b03591\n",
        "\n",
        "We are interested in predicting the hydrogen evolution rate, or \"HER\". But we also have data on the optical band gap \"Eg\" and the transmittance \"T\" of each polymer when dispersed in water.\n",
        "\n",
        "We will explore:\n",
        "\n",
        "How data skew is a problem in this dataset. This is a common materials problem: we typically want to focus on the rarer, higher performing materials, but these are scarce. We examine different approaches to handling this, including:\n",
        "  data augmentation through \"repeat measurements\"\n",
        "  using square root and log functions to reduce skew in the data\n",
        "  using a multi-task model that predicts multiple targets as once with different skews.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwfsGfIML2ix"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Get the list of sulfone polymers with experimental HERs.\n",
        "HER_polymers_url = \"https://raw.githubusercontent.com/S-AJ-H/Chemprop-Tutorial/main/1.2/HERs_sulfones.csv\"  #get data\n",
        "\n",
        "df_HER = pd.read_csv(HER_polymers_url) # Load into a DataFrame\n",
        "df_HER = df_HER[['MonA', 'MonB', 'Eg', 'HER', 'T']]\n",
        "\n",
        "# Convert 'Eg' column to numeric, coercing errors to NaN\n",
        "df_HER['Eg'] = pd.to_numeric(df_HER['Eg'], errors='coerce')\n",
        "# Drop rows where 'Eg' is NaN\n",
        "df_HER.dropna(subset=['Eg'], inplace=True)\n",
        "\n",
        "df_HER = df_HER[df_HER['HER'] != 0]     #remove HER = 0 values\n",
        "df_HER = df_HER[df_HER['T'] != 0]     #remove HER = 0 values\n",
        "\n",
        "df_HER['logHER'] = np.log(df_HER['HER']+1)  #create logHER  #+1 so the log plays nice - need to convert back later\n",
        "\n",
        "\n",
        "df_HER['sqrtHER'] = np.sqrt(df_HER['HER'])  #create sqrtHER\n",
        "df_HER['sqrtT'] = np.sqrt(df_HER['T'])  #create sqrtT\n",
        "df_HER['logHER'] = np.log(df_HER['HER']+1)  #create sqrtHER   #bodge so all are log\n",
        "df_HER['logT'] = np.log(df_HER['T']+1)  #create logT\n",
        "\n",
        "\n",
        "df_HER.dropna(subset=['MonA'], inplace=True)\n",
        "df_HER.reset_index(drop=True, inplace=True)\n",
        "#display(df_HER)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets look at the distributions of the three targets:"
      ],
      "metadata": {
        "id": "zhLySnolmybP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj_IQJXiAjVV"
      },
      "outputs": [],
      "source": [
        "#plot HER Eg and T as histograms:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
        "\n",
        "df_HER['HER'].plot(kind='hist', bins=20, ax=axes[0], title='HER')\n",
        "axes[0].spines[['top', 'right',]].set_visible(False)\n",
        "\n",
        "df_HER['Eg'].plot(kind='hist', bins=20, ax=axes[1], title='Eg')\n",
        "axes[1].spines[['top', 'right',]].set_visible(False)\n",
        "\n",
        "df_HER['T'].plot(kind='hist', bins=20, ax=axes[2], title='T')\n",
        "axes[2].spines[['top', 'right',]].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYpwY2ZWAWtm"
      },
      "outputs": [],
      "source": [
        "#plot log HER Eg and T as histograms\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
        "\n",
        "df_HER['logHER'].plot(kind='hist', bins=20, ax=axes[0], title='logHER')\n",
        "axes[0].spines[['top', 'right',]].set_visible(False)\n",
        "\n",
        "df_HER['Eg'].plot(kind='hist', bins=20, ax=axes[1], title='Eg')\n",
        "axes[1].spines[['top', 'right',]].set_visible(False)\n",
        "\n",
        "df_HER['logT'].plot(kind='hist', bins=20, ax=axes[2], title='logT')\n",
        "axes[2].spines[['top', 'right',]].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMyl4VWCecjH"
      },
      "outputs": [],
      "source": [
        "#plot sqrt HER Eg and T as histograms\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
        "\n",
        "df_HER['sqrtHER'].plot(kind='hist', bins=20, ax=axes[0], title='sqrtHER')\n",
        "axes[0].spines[['top', 'right',]].set_visible(False)\n",
        "\n",
        "df_HER['Eg'].plot(kind='hist', bins=20, ax=axes[1], title='Eg')\n",
        "axes[1].spines[['top', 'right',]].set_visible(False)\n",
        "\n",
        "df_HER['sqrtT'].plot(kind='hist', bins=20, ax=axes[2], title='sqrtT')\n",
        "axes[2].spines[['top', 'right',]].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets first try \"augmenting\" the data. The HER values have significant error attached - perhaps 10-30%. So it is reasonable to create extra data points by duplicating HER values with a small amount of noise. The code below preferentially adds extra data to the dataset which consists of the same SMILES but a HER with some amount of noise. This is akin to re-measuring the sample in the lab.\n",
        "\n",
        "What are the pros and cons of this approach?"
      ],
      "metadata": {
        "id": "T_quKiEPnK7R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4NBFhYtP2FU"
      },
      "outputs": [],
      "source": [
        "def augment_data(df, noise_percentage=0.01, top_n_percentage=0.2):\n",
        "  \"\"\"\n",
        "  Augments the HER data to have a flatter distribution by adding noise and duplicating high HER rows.\n",
        "\n",
        "  Args:\n",
        "    df: The input DataFrame containing 'HER' column.\n",
        "    noise_percentage: The percentage of noise (as a fraction of the HER value) to add.\n",
        "    top_n_percentage: The percentage of rows with the highest HER values to duplicate.\n",
        "\n",
        "  Returns:\n",
        "    A new DataFrame with augmented data.\n",
        "  \"\"\"\n",
        "  df_augmented = df.copy()\n",
        "\n",
        "  # Add noise to all rows\n",
        "  noise = np.random.normal(0, df_augmented['HER'] * noise_percentage, size=len(df_augmented))\n",
        "  df_augmented['HER'] = df_augmented['HER'] + noise\n",
        "  df_augmented['HER'] = df_augmented['HER'].clip(lower=0) # Ensure HER remains non-negative\n",
        "\n",
        "  # Duplicate rows with the highest HER values\n",
        "  top_n = int(len(df_augmented) * top_n_percentage)\n",
        "  df_sorted = df_augmented.sort_values(by='HER', ascending=False)\n",
        "  df_top_n = df_sorted.head(top_n)\n",
        "  df_augmented = pd.concat([df_augmented, df_top_n], ignore_index=True)\n",
        "\n",
        "  return df_augmented\n",
        "\n",
        "# Augment the data\n",
        "df_HER = augment_data(df_HER, noise_percentage=0.03, top_n_percentage=0.5)\n",
        "\n",
        "# Plot the distribution of the augmented data\n",
        "fig, axes = plt.subplots(1, 1, figsize=(5, 3))\n",
        "df_HER['HER'].plot(kind='hist', bins=20, ax=axes, title='Augmented HER Distribution')\n",
        "axes.spines[['top', 'right',]].set_visible(False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv3_BRwwct-T"
      },
      "outputs": [],
      "source": [
        "#for MonB = Oc1cc(O)c(Br)c(O)c1Br, plot all HER values by index\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "df_MonB = df_HER[df_HER['MonB'] == 'Oc1cc(O)c(Br)c(O)c1Br'].copy()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df_MonB.index, df_MonB['HER'], marker='o', linestyle='-')\n",
        "plt.title('HER values for MonB = Oc1cc(O)c(Br)c(O)c1Br by Index')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('HER')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAkTphqKHisA"
      },
      "outputs": [],
      "source": [
        "#define functions for doing chemistry with RDkit\n",
        "\n",
        "#function that deletes duplicates by converting from mols to *canonical* smiles and back to moles\n",
        "def rm_duplicate_mols(mols):\n",
        "    smiles = list(set([Chem.MolToSmiles(m, canonical=True) for m in mols]))\n",
        "    mols = [Chem.MolFromSmiles(s) for s in smiles]\n",
        "    return mols\n",
        "\n",
        "#Replaces Br with [At] in places where there is a cCBr (not just a cBr) - this \"protects\" cCBr\n",
        "def protect_CBr(m):\n",
        "    while m.HasSubstructMatch(Chem.MolFromSmarts('cCBr')):\n",
        "        smarts = \"[*:1]CBr>>[*:1]C[At]\"\n",
        "        rxn = AllChem.ReactionFromSmarts(smarts)\n",
        "        ps = rxn.RunReactants((m,))\n",
        "        products = rm_duplicate_mols([m[0] for m in ps])\n",
        "        m = products[0]\n",
        "    return m\n",
        "\n",
        "#reverses the protection once we're finished protecting\n",
        "def deprotect_CBr(m):\n",
        "    while m.HasSubstructMatch(Chem.MolFromSmarts('C[At]')):\n",
        "        smarts = \"[*:1]C[At]>>[*:1]CBr\"\n",
        "        rxn = AllChem.ReactionFromSmarts(smarts)\n",
        "        ps = rxn.RunReactants((m,))\n",
        "        products = rm_duplicate_mols([m[0] for m in ps])\n",
        "        m = products[0]\n",
        "    return m\n",
        "\n",
        "#the important one: remove the terminal groups\n",
        "def rm_termini(m):\n",
        "\n",
        "    # remove all Br\n",
        "    m = protect_CBr(m)\n",
        "    while m.HasSubstructMatch(Chem.MolFromSmarts('cBr')):\n",
        "        smarts = \"[*:1]Br>>[*:1]\"\n",
        "        rxn = AllChem.ReactionFromSmarts(smarts)\n",
        "        ps = rxn.RunReactants((m,))\n",
        "        products = rm_duplicate_mols([m[0] for m in ps])\n",
        "        m = products[0]\n",
        "    m = deprotect_CBr(m)\n",
        "\n",
        "    # remove all BOO\n",
        "    while m.HasSubstructMatch(Chem.MolFromSmarts('[B](-O)(-O)')):\n",
        "        smarts = \"[*:1]([B](-O)(-O))>>[*:1]\"\n",
        "        rxn = AllChem.ReactionFromSmarts(smarts)\n",
        "        ps = rxn.RunReactants((m,))\n",
        "        products = rm_duplicate_mols([m[0] for m in ps])  #converts mols to SMILES, sets all to canonical, converts back to mols. This prevents duplication\n",
        "        m = products[0]\n",
        "\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0smrZSgEPqJ"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "smiA_series = df_HER['MonA']\n",
        "smiB_series = df_HER['MonB']\n",
        "smiA = smiA_series.tolist()\n",
        "smiB = smiB_series.tolist()\n",
        "\n",
        "#Function that does the polymerisation\n",
        "def make_master_chemprop_input(smiA, smiB):\n",
        "    mA = Chem.MolFromSmiles(smiA)   #make mols from smiles\n",
        "    mB = Chem.MolFromSmiles(smiB)\n",
        "    mA = rm_termini(mA)             #remove end groups\n",
        "    mB = rm_termini(mB)\n",
        "    smiA = Chem.MolToSmiles(mA, canonical=True) #convert mol back to smiles, ensure all are canonical\n",
        "    smiB = Chem.MolToSmiles(mB, canonical=True)\n",
        "    smiles = f'{smiA}.{smiB}' #\"polymerise\" by joining the monomers together\n",
        "    return smiles\n",
        "\n",
        "df_HER.loc[:, 'Polymers'] = [make_master_chemprop_input(sA, sB) for sA, sB in zip(df_HER.loc[:, 'MonA'], df_HER.loc[:, 'MonB'])]\n",
        "#display(df_HER.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vYicChOnBNX"
      },
      "source": [
        "## 2a) Make molecule datapoints\n",
        "\n",
        "As in part 1.1, we define our SMILES and our targets - but this time, we define 3 targets simultaneously.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv1P6C1vnBNX"
      },
      "outputs": [],
      "source": [
        "#Get SMILES and targets\n",
        "smiles = df_HER.loc[:, 'MonB'].values\n",
        "target_columns = [\"HER\",\"T\",\"Eg\"]       # here we consider the 3 targets\n",
        "#target_columns = [\"HER\"]               # uncomment this line for single target mode\n",
        "\n",
        "targets = df_HER.loc[:, target_columns].values\n",
        "\n",
        "display(smiles[:2]) # show first 2 SMILES strings\n",
        "display(targets[:2]) # show first 2 targets\n",
        "\n",
        "#Use the SMILES to generate mol objects, pair the mol objects with the targets y\n",
        "all_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(smiles, targets)]\n",
        "display(all_data[:2])     #note this is different from the mol object we used before in RDKIt - the \"MoleculeDatapoint\" contains the mol *and* the targets, plus some extra customisation options that we're not going to use.\n",
        "\n",
        "#We can still access the molecules and extract chemical info if we like:\n",
        "#all_data[0].mol.GetNumAtoms()   #Can use RDKit things on the mol, but not on the MoleculeDatapoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq_WbgLmnBNY"
      },
      "source": [
        "## 2b) Perform data splitting for training, validation, and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FevWp_9LnBNY"
      },
      "source": [
        "ChemProp's `make_split_indices` function will always return a two- (if no validation) or three-length tuple (if including validation, like in this example). The inner lists then contain the actual indices for splitting.\n",
        "\n",
        "The type signature for this return type is `tuple[list[list[int]], ...]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5paMtwC8nBNY"
      },
      "outputs": [],
      "source": [
        "# available split types - Kennard Stone, Kmeans and scafford balanced are all structure-based splits (similar structures go to same split) based on https://jacksonburns.github.io/astartes/\n",
        "#list(data.SplitType.keys())\n",
        "\n",
        "mols = [d.mol for d in all_data]  # RDkit Mol objects are use for structure based splits\n",
        "train_indices, val_indices, test_indices = data.make_split_indices(mols, \"KENNARD_STONE\", (0.8, 0.1, 0.1))  # unpack the tuple into three separate lists. data is a Chemprop function\n",
        "\n",
        "#display(test_indices[0]) #test indices\n",
        "display(\"test_indices length =\", len(test_indices[0]))     #~6138/10. (Note that the list of test indices is nested)\n",
        "\n",
        "train_data, val_data, test_data = data.split_data_by_indices(\n",
        "    all_data, train_indices, val_indices, test_indices)     #Use the 3 lists of indices to split the data.\n",
        "\n",
        "#display(len(test_data[0]))\n",
        "#display(test_data[0][0])  #same format as in 2a\n",
        "display(test_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evggn0vsZrPz"
      },
      "source": [
        "## 2c) Make the three MoleculeDatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGccGzfyX_pk"
      },
      "outputs": [],
      "source": [
        "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer() #chemprop module which outputs a MolGraph\n",
        "\n",
        "train_dset = data.MoleculeDataset(train_data[0], featurizer)  #append the type of featuriser to the list of molecule to make the molecule dataset. (We use train_data[0] because of the aforementioned nesting)\n",
        "#The MoleculeDataset \"train_dset\" is a list of MoleculeDatapoints, with the featurizer at the end:\n",
        "#display(len(test_dset))\n",
        "#display(test_dset)       #this should give MoleculeDataset(data=[MoleculeDatapoint(mol=<rdkit.Chem.rdchem.Mol object at 0x7f7a6b6ed5b0>, y=array([0.23384385]), weight=1.0, gt_mask=None, lt_mask=None, x_d=None, x_phase=None, name='C', V_f=None, E_f=None, V_d=None), MoleculeDatapoint(mol=<rdkit.Chem.rdchem.Mol object at 0x7f7a6b6ed690>, y=array([0.74433064]), weight=1.0, gt_mask=None, lt_mask=None, x_d=None, x_phase=None, name='CC', V_f=None, E_f=None, V_d=None)], featurizer=SimpleMoleculeMolGraphFeaturizer(atom_featurizer=<chemprop.featurizers.atom.MultiHotAtomFeaturizer object at 0x7f7a6b52c290>, bond_featurizer=<chemprop.featurizers.bond.MultiHotBondFeaturizer object at 0x7f7a6b52c150>))\n",
        "#display(test_dset.data[:1])      #test_dset.data is the same as test_data, without nesting;\n",
        "\n",
        "#each MoleculeDatapoint now has MolGraph features which are accessed through indexing. Compare to Part (4) - its the same but we have MolGraph features instead of mol objects.\n",
        "#display(test_dset[0])\n",
        "\n",
        "scaler = train_dset.normalize_targets() #define the normalisation using StandardScaler (subtract mean, scale to unit variance)\n",
        "\n",
        "\n",
        "#Do the same for validation and test\n",
        "val_dset = data.MoleculeDataset(val_data[0], featurizer)\n",
        "val_dset.normalize_targets(scaler)            #normalise the validation dataset in the same way as the training\n",
        "\n",
        "test_dset = data.MoleculeDataset(test_data[0], featurizer)      #no normalisation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBHPjwXjkTvw"
      },
      "outputs": [],
      "source": [
        "#display the train_dset y values in a graph\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Extract y values from train_dset\n",
        "train_y_values = [datapoint.y[0] for datapoint in train_dset]\n",
        "\n",
        "# Create a histogram of the y values\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.hist(train_y_values, bins=20, edgecolor='black')\n",
        "plt.title('Distribution of train_dset y values (HER)')\n",
        "plt.xlabel('HER')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "# If you also want to see the distribution for the second target (Transmission)\n",
        "train_y_trans_values = [datapoint.y[1] for datapoint in train_dset]\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.hist(train_y_trans_values, bins=20, edgecolor='black')\n",
        "plt.title('Distribution of train_dset y values (Transmission)')\n",
        "plt.xlabel('Transmission')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "#and Eg\n",
        "train_y_trans_values = [datapoint.y[2] for datapoint in train_dset]\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.hist(train_y_trans_values, bins=20, edgecolor='black')\n",
        "plt.title('Distribution of train_dset y values (Eg)')\n",
        "plt.xlabel('Eg')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GEuFA8GnBNY"
      },
      "source": [
        "# 3) Message-Passing Neural Network input parameters\n",
        "\n",
        ">Now our data is ready, its time to use Chemprop.There are 3 main steps: Message passing, aggregation and the feed-forward NN.\n",
        "\n",
        "> For more info on a step: https://chemprop.readthedocs.io/en/latest/tutorial/python/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-3oQW3ynBNY"
      },
      "source": [
        "## 3a) Message Passing and Aggregation\n",
        "\n",
        "In `nn.MeanAggregation()`, we can set the number of tasks to make a multi-task predictor.\n",
        "\n",
        "Try changing the learning rates!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqD93OQlyIQq"
      },
      "outputs": [],
      "source": [
        "#Message passing: construct node-level representations of the atoms\n",
        "#Define message passing type. Can pass different activations, dropout, etc etc.\n",
        "#Defaults: 300 hidden dimensions. 3 message passing iterations. ReLU.\n",
        "dim = 300\n",
        "mp = nn.BondMessagePassing(d_h=dim, dropout=0.1)    #https://chemprop.readthedocs.io/en/latest/autoapi/chemprop/nn/index.html#chemprop.nn.BondMessagePassing\n",
        "\n",
        "#Aggregation: node-level --> graph-level representation\n",
        "agg = nn.MeanAggregation()    #average together all of the hidden node/edge representations to get a graph-level representation.\n",
        "#Note this is the only place where the two monomers in each pair interact!\n",
        "batch_norm = True #normalizes the outputs of the aggregation by re-centering and re-scaling. Helps keep the inputs to the FFN small and centered around zero.\n",
        "\n",
        "\n",
        "output_transform = nn.UnscaleTransform.from_standard_scaler(scaler) #\"un-scale\" the data\n",
        "\n",
        "#Regression options include:\n",
        "#ffn = nn.RegressionFFN()`\n",
        "#ffn = nn.MveFFN()`\n",
        "#ffn = nn.EvidentialFFN()`\n",
        "\n",
        "#define the feed-forward network:\n",
        "ffn = nn.RegressionFFN(output_transform=output_transform, input_dim = dim, hidden_dim = 300, dropout=0.1, n_tasks = 2) #n_tasks sets the number of targets. Can change number of layers etc etc.\n",
        "#set the metrics:\n",
        "\n",
        "metric_list = [nn.metrics.RMSE(),nn.metrics.MAE(), nn.metrics.R2Score()] # Only the first metric is used for training and early stopping\n",
        "\n",
        "mpnn = models.MPNN(mp, agg, ffn, batch_norm, metric_list, init_lr = 1e-5, max_lr = 5e-5) #can change learning rate, optimiser etc\n",
        "#mpnn\n",
        "#Entire model, consisting of message passing, aggregation and FFN, is end-to-end trained.\n",
        "\n",
        "#In the message passing NN:\n",
        "#w_i = input weights, applied to the bond feature vectors. Length = sum of bond and atom features. Output hidden dimension is 300 by default.\n",
        "#w_h = hidden weights, applied to the messages.\n",
        "#w_o = output weights.\n",
        "\n",
        "#In the FFN:\n",
        "#Input dimension is the same as the MPNN hidden dimension (300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siESRK6CnBNd"
      },
      "source": [
        "# 4) Set up and start the trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qanCRbNRnBNd"
      },
      "outputs": [],
      "source": [
        "#Get DataLoader\n",
        "\n",
        "num_workers = 0 # number of workers for dataloader. 0 means using main process for data loading. Specifies how many subprocesses should be used to load data.\n",
        "#Each of these subprocesses retrieves a batch of data from your dataset and sends it to the main training process.\n",
        "\n",
        "train_loader = data.build_dataloader(train_dset, num_workers=num_workers)\n",
        "val_loader = data.build_dataloader(val_dset, num_workers=num_workers, shuffle=False)\n",
        "test_loader = data.build_dataloader(test_dset, num_workers=num_workers, shuffle=False)\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "# Configure model checkpointing\n",
        "checkpointing = ModelCheckpoint(\n",
        "    \"checkpoints\",  # Directory where model checkpoints will be saved\n",
        "    \"best-{epoch}-{val_loss:.2f}\",  # Filename format for checkpoints, including epoch and validation loss\n",
        "    \"val_loss\",  # Metric used to select the best checkpoint (based on validation loss)\n",
        "    mode=\"min\",  # Save the checkpoint with the lowest validation loss (minimization objective)\n",
        "    save_last=True,  # Always save the most recent checkpoint, even if it's not the best\n",
        ")\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "from lightning.pytorch.loggers import CSVLogger\n",
        "\n",
        "# Configure CSV logger\n",
        "logger = CSVLogger(\"/content/chemprop/\", name=\"losses\")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=logger,\n",
        "    enable_checkpointing=True, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
        "    enable_progress_bar=True,\n",
        "    accelerator=\"auto\",\n",
        "    devices=1,\n",
        "    max_epochs=500, # number of epochs to train for\n",
        "    callbacks=[checkpointing], # Use the configured checkpoint callback\n",
        ")\n",
        "\n",
        "\n",
        "#start training\n",
        "trainer.fit(mpnn, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ebdbmalnBNe"
      },
      "source": [
        "# 5) Test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2W_an7InBNe"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/chemprop/losses/version_16/metrics.csv\")    #change version number starting at 0 each time you restart colab\n",
        "\n",
        "df['train_loss_epoch'] = df['train_loss_epoch'].shift(-1)\n",
        "df.dropna(subset=['train_loss_epoch'], inplace=True)\n",
        "\n",
        "#display(df)\n",
        "\n",
        "\n",
        "# Plot val_loss and train_loss_epoch\n",
        "plt.plot(df[\"epoch\"], df[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(df[\"epoch\"], df[\"train_loss_epoch\"], label=\"train_loss_epoch\")\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xlim(0, 500)\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "results = trainer.test(dataloaders=test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTgbvval2uK0"
      },
      "outputs": [],
      "source": [
        "full_dset = data.MoleculeDataset(all_data, featurizer=featurizer)\n",
        "full_loader = data.build_dataloader(full_dset, shuffle=False)\n",
        "\n",
        "predictions = trainer.predict(mpnn, full_loader)\n",
        "predictions_array = np.concatenate(predictions, axis=0)\n",
        "#append the predicted values to the original array\n",
        "\n",
        "df_HER[['pred_HER']] = predictions_array\n",
        "#df_HER[['pred_HER', 'pred_T', 'pred_Eg']] = predictions_array\n",
        "\n",
        "#df_HER[['pred_HER']] = df_HER[['sqrt_pred_HER']]*df_HER[['sqrt_pred_HER']]\n",
        "#df_HER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo85FpHe7xAw"
      },
      "source": [
        "Plot prediction vs actual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkqILBHV7u3d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "\n",
        "# Plot pred vs true\n",
        "plt.scatter(df_HER['HER'], df_HER['pred_HER'], label='Prediction', marker='x', color='red')\n",
        "\n",
        "# Plot y=x\n",
        "plt.plot(df_HER['HER'], df_HER['HER'], label='y=x', linestyle='-')\n",
        "\n",
        "# Filter df_HER for rows corresponding to test_indices and plot with green circles\n",
        "test_indices_flat = test_indices[0]\n",
        "df_test = df_HER.iloc[test_indices_flat]\n",
        "\n",
        "plt.scatter(df_test['HER'], df_test['pred_HER'], label='Test Prediction', marker='o', color='green')\n",
        "\n",
        "plt.xlabel('true HER')\n",
        "plt.ylabel('Predicted HER')\n",
        "plt.title('Pred vs true HER')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuYRVFrD1P65"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "\n",
        "# Plot pred vs true\n",
        "plt.scatter(df_HER['T'], df_HER['pred_T'], label='Prediction', marker='x', color='red')\n",
        "\n",
        "# Plot y=x\n",
        "plt.plot(df_HER['T'], df_HER['T'], label='y=x', linestyle='-')\n",
        "\n",
        "plt.xlabel('true T')\n",
        "plt.ylabel('Predicted T')\n",
        "plt.title('Pred vs true T')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DwCBf7RUflx"
      },
      "outputs": [],
      "source": [
        "# Plot prediction vs actual Eg\n",
        "plt.figure(figsize=(5, 3))\n",
        "\n",
        "# Plot pred vs true\n",
        "plt.scatter(df_HER['Eg'], df_HER['pred_Eg'], label='Prediction', marker='x', color='red')\n",
        "\n",
        "# Plot y=x\n",
        "plt.plot(df_HER['Eg'], df_HER['Eg'], label='y=x', linestyle='-')\n",
        "\n",
        "plt.xlabel('true Eg')\n",
        "plt.ylabel('Predicted Eg')\n",
        "plt.title('Pred vs true Eg')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_zuK_pRYtaD"
      },
      "source": [
        "# 6) Bonus: Transfer learning\n",
        "\n",
        "Transfer learning (or pretraining) leverages knowledge from a pre-trained model on a related task to enhance performance on a new task. In Chemprop, we can use pre-trained model checkpoints to initialize a new model and freeze components of the new model during training.\n",
        "\n",
        "By repeating the steps in 1.1, pre-train a GNN using the EA and/or IP data. Then use the data in 1.2 and the info below to fine-tune using the experimental HER data.\n",
        "\n",
        "Can you train a model to a similar level of accuracy to the one above? Why and/or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2PCnUHOegAg"
      },
      "source": [
        "## 6a) Make the MoleculeDatasets by following the steps in Section 2.\n",
        "\n",
        "> The previously-trained model was trained on a scaled dataset. The scaler is saved as part of the model and used during prediction. For further training, we need to scale the fine-tuning data with the same target scaler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZpXZzzSd1Vs"
      },
      "outputs": [],
      "source": [
        "#2a\n",
        "#Get SMILES and targets\n",
        "df_HER = df_HER.replace('ND', pd.NA)\n",
        "df_HER = df_HER.dropna()\n",
        "\n",
        "display(df_HER)\n",
        "\n",
        "smiles_HER = df_HER.loc[:, 'df_HER_polymers'].values\n",
        "targets_HER = df_HER.loc[:,['sqrtHER']].values       #using sqrtHER as better distributed\n",
        "\n",
        "#display(smiles_HER[:2]) # show first 2 SMILES strings\n",
        "#display(targets_HER[:2]) # show first 2 targets\n",
        "\n",
        "#Use the SMILES to generate mol objects, pair the mol objects with the targets y\n",
        "HER_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(smiles_HER, targets_HER)]\n",
        "#display(HER_data[:2])\n",
        "\n",
        "\n",
        "#2b\n",
        "mols_HER = [d.mol for d in HER_data]  # RDkit Mol objects are use for structure based splits\n",
        "train_indices_HER, val_indices_HER, test_indices_HER = data.make_split_indices(mols_HER, \"random\", (0.1, 0.1, 0.8))  # unpack the tuple into three separate lists. data is a Chemprop function\n",
        "\n",
        "#display(\"test_indices length =\", len(test_indices_HER[0]))     #~6138/10. (Note that the list of test indices is nested)\n",
        "\n",
        "train_data_HER, val_data_HER, test_data_HER = data.split_data_by_indices(\n",
        "    HER_data, train_indices_HER, val_indices_HER, test_indices_HER)     #Use the 3 lists of indices to split the data.\n",
        "\n",
        "#new bit\n",
        "checkpoint_path = \"/content/chemprop/checkpoints/best-epoch=8-val_loss=0.03.ckpt\"     ##Put the name of your best checkpoint here! These are the learned mpnn parameters from the earlier training.\n",
        "#checkpoint_path = \"https://raw.githubusercontent.com/S-AJ-H/Chemprop-Tutorial/main/best-epoch=9-val_loss=0.04.ckpt\"\n",
        "mpnn_cls = models.MPNN\n",
        "mpnn = mpnn_cls.load_from_file(checkpoint_path)\n",
        "\n",
        "#display(mpnn)\n",
        "\n",
        "#new bit: scaling\n",
        "#pretraining_scaler = scaler\n",
        "#pretraining_scaler.mean_ = mpnn.predictor.output_transform.mean.numpy()\n",
        "#pretraining_scaler.scale_ = mpnn.predictor.output_transform.scale.numpy()\n",
        "\n",
        "#2c\n",
        "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
        "\n",
        "train_dset_HER = data.MoleculeDataset(train_data_HER[0], featurizer)\n",
        "#train_dset_HER.normalize_targets(pretraining_scaler)\n",
        "scaler_HER = train_dset_HER.normalize_targets()\n",
        "\n",
        "\n",
        "val_dset_HER = data.MoleculeDataset(val_data_HER[0], featurizer)\n",
        "#val_dset_HER.normalize_targets(pretraining_scaler)\n",
        "val_dset_HER.normalize_targets(scaler_HER)\n",
        "test_dset_HER = data.MoleculeDataset(test_data_HER[0], featurizer)\n",
        "\n",
        "#define the new scaling for the feed-forward network:\n",
        "output_transform_HER = nn.UnscaleTransform.from_standard_scaler(scaler_HER) #\"un-scale\" the data\n",
        "mpnn.predictor.output_transform = output_transform_HER\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "df_HER['sqrtHER_sscale'] = StandardScaler().fit_transform(df_HER[['sqrtHER']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpEYIWiM9dDo"
      },
      "outputs": [],
      "source": [
        "#look at the data distributions\n",
        "import seaborn as sns\n",
        "# Original\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.histplot(df_HER['HER'], bins=30, kde=True)\n",
        "plt.title('Original Skewed HER')\n",
        "\n",
        "# Log-transformed\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.histplot(df_HER['sqrtHER'], bins=30, kde=True)\n",
        "plt.title('After Log Transformation')\n",
        "\n",
        "# Standard Scaled (after log)\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.histplot(df_HER['sqrtHER_sscale'], bins=30, kde=True)\n",
        "plt.title('After dqrt + StandardScaler')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9UXMiGMdIFJ"
      },
      "source": [
        "## 6b) Freezing MPNN and FFN layers\n",
        "Certain layers of a pre-trained model can be kept unchanged during further training on a new task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlyBAmm2dIFJ"
      },
      "outputs": [],
      "source": [
        "#To freeze the MPNN (i.e. the learned representation of the molecules)\n",
        "mpnn.message_passing.apply(lambda module: module.requires_grad_(False))\n",
        "mpnn.message_passing.eval()\n",
        "mpnn.bn.apply(lambda module: module.requires_grad_(False))\n",
        "mpnn.bn.eval()  # Set batch norm layers to eval mode to freeze running mean and running var.\n",
        "\n",
        "#To freeze the FFN\n",
        "#frzn_ffn_layers = 1  # the number of consecutive FFN layers to freeze.\n",
        "#for idx in range(frzn_ffn_layers):\n",
        "#   mpnn.predictor.ffn[idx].requires_grad_(False)\n",
        "#   mpnn.predictor.ffn[idx + 1].eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FpV1is_gCmS"
      },
      "source": [
        "## 6c) Train and test by following the steps in Sections 4 and 5.\n",
        "\n",
        "> Are the predictions for HER better or worse than for EA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPE9oUn0gLNL"
      },
      "outputs": [],
      "source": [
        "#4\n",
        "#Get DataLoader\n",
        "\n",
        "num_workers = 0 # number of workers for dataloader. 0 means using main process for data loading. Specifies how many subprocesses should be used to load data.\n",
        "#Each of these subprocesses retrieves a batch of data from your dataset and sends it to the main training process.\n",
        "\n",
        "train_loader_HER = data.build_dataloader(train_dset_HER, num_workers=num_workers)\n",
        "val_loader_HER = data.build_dataloader(val_dset_HER, num_workers=num_workers, shuffle=False)\n",
        "test_loader_HER = data.build_dataloader(test_dset_HER, num_workers=num_workers, shuffle=False)\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "# Configure model checkpointing\n",
        "checkpointing = ModelCheckpoint(\n",
        "    \"checkpoints\",  # Directory where model checkpoints will be saved\n",
        "    \"best-HER-{epoch}-{val_loss:.2f}\",  # Filename format for checkpoints, including epoch and validation loss\n",
        "    \"val_loss\",  # Metric used to select the best checkpoint (based on validation loss)\n",
        "    mode=\"min\",  # Save the checkpoint with the lowest validation loss (minimization objective)\n",
        "    save_last=True,  # Always save the most recent checkpoint, even if it's not the best\n",
        ")\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=False,\n",
        "    enable_checkpointing=True, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
        "    enable_progress_bar=True,\n",
        "    accelerator=\"auto\",\n",
        "    devices=1,\n",
        "    max_epochs=200, # number of epochs to train for\n",
        "    callbacks=[checkpointing], # Use the configured checkpoint callback\n",
        ")\n",
        "\n",
        "\n",
        "#start training\n",
        "mpnn.train()\n",
        "trainer.fit(mpnn, train_loader_HER, val_loader_HER)\n",
        "#5\n",
        "results = trainer.test(dataloaders=test_loader_HER) #notice how there are fewer trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ-XlPVhv1iI"
      },
      "outputs": [],
      "source": [
        "full_dset_HER = data.MoleculeDataset(HER_data, featurizer=featurizer)\n",
        "full_loader_HER = data.build_dataloader(full_dset_HER, shuffle=False)\n",
        "\n",
        "predictions_HER = trainer.predict(mpnn, full_loader_HER)\n",
        "\n",
        "#append the predicted values to the original array\n",
        "predictions_array_HER = np.concatenate(predictions_HER, axis=0)\n",
        "df_HER[['sqrt_pred_HER']] = predictions_array_HER\n",
        "df_HER['pred_HER'] = np.square(df_HER['sqrt_pred_HER'])\n",
        "df_HER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhfjiJgBv43Q"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot pred vs true\n",
        "plt.scatter(df_HER['HER'], df_HER['pred_HER'], label='Prediction', marker='x', color='red')\n",
        "\n",
        "# Plot y=x\n",
        "plt.plot(df_HER['HER'], df_HER['HER'], label='y=x', linestyle='-')\n",
        "\n",
        "plt.xlabel('true HER')\n",
        "plt.ylabel('Predicted HER')\n",
        "plt.title('Pred vs true HER, transferEAandIP, freeze_MPNN, sqrt_transform')\n",
        "plt.xlim(0, 10000)    # x-axis\n",
        "plt.ylim(0, 10000)  # y-axis\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3khTEFSKnBNS",
        "Q_zuK_pRYtaD",
        "y2PCnUHOegAg"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}